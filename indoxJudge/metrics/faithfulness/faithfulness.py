from typing import List
from pydantic import BaseModel, Field
import json

from .template import FaithfulnessTemplate
import json

from loguru import logger
import sys

# Set up logging
logger.remove()  # Remove the default logger
logger.add(
    sys.stdout, format="<green>{level}</green>: <level>{message}</level>", level="INFO"
)

logger.add(
    sys.stdout, format="<red>{level}</red>: <level>{message}</level>", level="ERROR"
)


class FaithfulnessVerdict(BaseModel):
    """
    Model representing a verdict on the faithfulness of a claim,
    including the verdict itself and the reasoning behind it.
    """

    verdict: str
    reason: str = Field(default=None)


class Verdicts(BaseModel):
    """
    Model representing a list of FaithfulnessVerdict instances.
    """

    verdicts: List[FaithfulnessVerdict]


class Truths(BaseModel):
    """
    Model representing a list of truths extracted from the LLM response.
    """

    truths: List[str]


class Claims(BaseModel):
    """
    Model representing a list of claims extracted from the LLM response.
    """

    claims: List[str]


class Reason(BaseModel):
    """
    Model representing the reason provided for a given score or set of contradictions.
    """

    reason: str


class Faithfulness:
    """
    Class for evaluating the faithfulness of language model outputs by analyzing
    claims, truths, verdicts, and reasons using a specified language model.
    """

    def __init__(self, llm_response, retrieval_context):
        """
        Initializes the Faithfulness class with the LLM response and retrieval context.

        :param llm_response: The response generated by the language model.
        :param retrieval_context: The context used for retrieval during evaluation.
        """
        self.model = None
        self.llm_response = llm_response
        self.retrieval_context = retrieval_context
        self.total_output_tokens = 0
        self.total_input_tokens = 0

    def set_model(self, model):
        """
        Sets the language model to be used for evaluation.

        :param model: The language model to use.
        """
        self.model = model

    def _clean_json_response(self, response: str) -> str:
        """
        Cleans the JSON response from the language model by removing markdown code blocks if present.

        :param response: Raw response from the language model
        :return: Cleaned JSON string
        """
        if response.startswith("```json") and response.endswith("```"):
            response = response[7:-3].strip()
        return response

    def evaluate_claims(self) -> Claims:
        """
        Evaluates and extracts claims from the LLM response.

        :return: A Claims object containing the list of claims.
        """
        prompt = FaithfulnessTemplate.generate_claims(self.llm_response)
        response = self._call_language_model(prompt)
        cleaned_response = self._clean_json_response(response)
        claims = json.loads(cleaned_response).get("claims", [])
        return Claims(claims=claims)

    def evaluate_truths(self) -> Truths:
        """
        Evaluates and extracts truths from the LLM response.

        :return: A Truths object containing the list of truths.
        """
        prompt = FaithfulnessTemplate.generate_truths(self.llm_response)
        response = self._call_language_model(prompt)
        cleaned_response = self._clean_json_response(response)
        truths = json.loads(cleaned_response).get("truths", [])
        return Truths(truths=truths)

    def evaluate_verdicts(self, claims: List[str]) -> Verdicts:
        """
        Evaluates the verdicts on the faithfulness of the given claims.

        :param claims: List of claims to evaluate.
        :return: A Verdicts object containing the list of verdicts.
        """
        prompt = FaithfulnessTemplate.generate_verdicts(claims, self.retrieval_context)
        response = self._call_language_model(prompt)
        cleaned_response = self._clean_json_response(response)
        verdicts = json.loads(cleaned_response).get("verdicts", [])
        return Verdicts(
            verdicts=[FaithfulnessVerdict(**verdict) for verdict in verdicts]
        )

    def evaluate_reason(self, score: float, contradictions: List[str]) -> Reason:
        """
        Evaluates the reason behind a given score or set of contradictions.

        :param score: The score assigned to the evaluation.
        :param contradictions: A list of contradictions identified.
        :return: A Reason object containing the reasoning.
        """
        prompt = FaithfulnessTemplate.generate_reason(score, contradictions)
        response = self._call_language_model(prompt)
        cleaned_response = self._clean_json_response(response)
        reason = json.loads(cleaned_response).get("reason", "")
        return Reason(reason=reason)

    def calculate_faithfulness_score(self) -> float:
        """
        Enhanced implementation that leverages all available methods:
        1. Gets claims using evaluate_claims()
        2. Gets truths using evaluate_truths() for additional verification
        3. Evaluates verdicts using evaluate_verdicts()
        4. Generates final reasoning using evaluate_reason()
        """
        # Get claims and truths
        claims = self.evaluate_claims().claims
        truths = self.evaluate_truths().truths
        if not claims:
            return {
                "score": 1,
                "reason": "",
                "claims": "",
                "truths": "",
                "verdicts": "",
            }
        # Evaluate verdicts
        verdicts = self.evaluate_verdicts(claims).verdicts

        # Calculate score
        faithful_count = sum(1 for verdict in verdicts if verdict.verdict == "yes")
        score = faithful_count / len(claims)
        # Get contradictions for reasoning
        contradictions = [
            verdict.reason
            for verdict in verdicts
            if verdict.verdict == "no" and verdict.reason
        ]

        # Generate final reasoning
        final_reason = self.evaluate_reason(score, contradictions)
        logger.info(
            f"Token Usage Summary:\n Total Input: {self.total_input_tokens} | Total Output: {self.total_output_tokens} | Total: {self.total_input_tokens + self.total_output_tokens}"
        )
        return {
            "score": score,
            "reason": final_reason,
            "claims": ", ".join(claims),
            "truths": ", ".join(truths),
            "verdicts": verdicts,
        }

    def _call_language_model(self, prompt: str) -> str:
        import tiktoken

        enc = tiktoken.get_encoding("cl100k_base")
        input_token_count = len(enc.encode(prompt))
        response = self.model.generate_evaluation_response(prompt=prompt)
        self.total_input_tokens += input_token_count

        if not response:
            raise ValueError("Received an empty response from the model.")

        clean_response = self._clean_json_response(response=response)
        output_token_count = len(enc.encode(response))
        self.total_output_tokens += output_token_count
        logger.info(
            f"Token Counts - Input: {input_token_count} | Output: {output_token_count}"
        )

        return clean_response
