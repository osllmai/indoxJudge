{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683cbd8af3980e07",
   "metadata": {},
   "source": [
    "# RAG Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413162f477c5781",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDoxJudge/blob/main/examples/rag_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59547c8b03b3fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indoxJudge -U\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259ce9e5cffc8c0",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxJudge`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxJudge\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxJudge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxJudge\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxJudge/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:16:01.767205Z",
     "start_time": "2024-09-14T16:16:01.741106Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c243c048e503e90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:16:01.772203Z",
     "start_time": "2024-09-14T16:16:01.767205Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What are the benefits of diet?\"\n",
    "retrieval_context = [\"The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.\",\"Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer's disease. The diet's high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.\",\"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition.\"]\n",
    "response = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer's disease, promoting longevity and overall well-being.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65529ced4f6beb0b",
   "metadata": {},
   "source": [
    "## Importing Required Modules\n",
    "imports the necessary classes from the indoxJudge library. The RAGevaluator class is used for evaluating retrieval-augmented generation models, assessing their performance based on various metrics. Additionally, OpenAi is the class used to interact with OpenAI models, such as GPT-3.5, providing the foundational language model capabilities for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57511dda37de77a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:16:03.935340Z",
     "start_time": "2024-09-14T16:16:01.772203Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxJudge.pipelines import RagEvaluator\n",
    "from indoxJudge.models import OpenAi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14599bd20a3d8f5d",
   "metadata": {},
   "source": [
    "## Initializing the OpenAI Model\n",
    "Here, the OpenAi class is instantiated to create a model object that interacts with OpenAI's gpt-3.5-turbo-0125 model. The api_key is passed to authenticate the API request. Replace OPENAI_API_KEY with your actual API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f0ad7b798f17c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:16:03.940109Z",
     "start_time": "2024-09-14T16:16:03.935340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-4o-mini and max_tokens: 2048\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model\n",
    "# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o. or other models\n",
    "\n",
    "model = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b557f91707091",
   "metadata": {},
   "source": [
    "The RAGevaluator class is instantiated to create an evaluator object. This object is configured to assess the modelâ€™s performance by using a specific response, retrieval context, and query. The llm_as_judge parameter is set to the model created in the previous step, while llm_response, retrieval_context, and query are additional components utilized during the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d8bf90524259fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:16:08.126927Z",
     "start_time": "2024-09-14T16:16:03.940109Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = RagEvaluator(llm_as_judge=model,llm_response=response,retrieval_context=retrieval_context,query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2ae0a78d3b70",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Running the Evaluation:** This line calls the `judge` method on the `evaluator` object. The `judge` method runs through all the specified metrics (e.g., Faithfulness, Answer Relevancy, Hallucination, GEval, Knowledge Retention, BertScore, METEOR) to evaluate the language model's performance.\n",
    "  \n",
    "- **Logging the Process:** As the evaluation runs, the process logs the start and completion of each metric evaluation, providing feedback on the progress. Each log entry is tagged with an INFO level, indicating routine operational messages.\n",
    "  \n",
    "- **Handling Warnings:** You may notice a warning regarding model initialization and a future deprecation notice from the Hugging Face Transformers library. These warnings inform you about potential issues related to model compatibility and upcoming changes in the library.\n",
    "  \n",
    "- **Completion of Metrics:** After all metrics have been evaluated, the `judge` method completes, and the evaluation results are stored in the `eval_result` dictionary for further analysis or reporting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af324b3695562eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:17:19.261943Z",
     "start_time": "2024-09-14T16:16:09.413628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mModel set for all metrics.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mRagEvaluator initialized with model and metrics.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Faithfulness\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 257 | Output: 172\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 222 | Output: 174\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 893 | Output: 181\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 240 | Output: 45\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1612 | Total Output: 572 | Total: 2184\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Faithfulness, score: 0.93\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: AnswerRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 198 | Output: 153\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 524 | Output: 168\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 196 | Output: 35\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 918 | Total Output: 356 | Total: 1274\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: AnswerRelevancy, score: 1.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: ContextualRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 346 | Output: 46\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 344 | Output: 36\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 322 | Output: 40\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 346 | Output: 48\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 344 | Output: 37\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 322 | Output: 40\u001b[0m\n",
      "verdicts=[ContextualRelevancyVerdict(verdict='yes', reason='The context provides relevant information about the Mediterranean diet, which is a type of diet that emphasizes plant-based foods and healthy fats, directly relating to the benefits of diet.'), ContextualRelevancyVerdict(verdict='yes', reason='The context provides specific benefits of the Mediterranean diet, which directly addresses the input question about the benefits of diet.'), ContextualRelevancyVerdict(verdict='yes', reason='The context discusses the benefits of a Mediterranean diet, which directly relates to the input question about the benefits of diet in general.')]\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 204 | Output: 40\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 2228 | Total Output: 287 | Total: 2515\u001b[0m\n",
      "{'verdicts': [{'verdict': 'yes', 'reason': 'The context provides relevant information about the Mediterranean diet, which is a type of diet that emphasizes plant-based foods and healthy fats, directly relating to the benefits of diet.'}, {'verdict': 'yes', 'reason': 'The context provides specific benefits of the Mediterranean diet, which directly addresses the input question about the benefits of diet.'}, {'verdict': 'yes', 'reason': 'The context discusses the benefits of a Mediterranean diet, which directly relates to the input question about the benefits of diet in general.'}], 'reason': {'reason': 'The score is 1.0 because the input is perfectly aligned with the topic of diet benefits, and there are no irrelevant contexts to detract from its relevance!'}, 'score': 1.0}\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: ContextualRelevancy, score: 1.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: GEval\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 270 | Output: 100\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 539 | Output: 59\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 809 | Total Output: 159 | Total: 968\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: GEval, score: 0.8\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Hallucination\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 628 | Output: 192\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 272 | Output: 39\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 900 | Total Output: 231 | Total: 1131\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Hallucination, score: 0.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: KnowledgeRetention\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 310 | Output: 16\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 562 | Output: 10\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 144 | Output: 33\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1016 | Total Output: 59 | Total: 1075\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: KnowledgeRetention, score: 1.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BertScore\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m\n",
      "Completed evaluation for metric: BertScore, scores: \n",
      "precision: 0.74,\n",
      "recall: 0.77,\n",
      "f1_score: 0.75,\n",
      "                        \u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: METEOR\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: METEOR, score: 0.76\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluation Completed, Check out the results\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.judge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225bcfef25a2bd5c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Retrieving Metric Scores:** The first line assigns the detailed metric scores from the evaluation to the variable `evaluator_metrics_score`. These scores reflect the performance of the language model across individual metrics such as Faithfulness, Answer Relevancy, Hallucination, GEval, Knowledge Retention, BertScore, METEOR, Recall, F1 Score.\n",
    "\n",
    "- **Retrieving Overall Evaluation Score:** The second line assigns the overall evaluation score to the variable `evaluator_evaluation_score`. This score is typically a cumulative measure that reflects the model's performance across all evaluated metrics.\n",
    "\n",
    "- **Example Output:** The dictionary shown represents a typical output of `evaluator_metrics_score`. Each key corresponds to a specific metric, and the associated value indicates the model's score for that metric. For instance, a `1.0` score in `AnswerRelevancy` and `KnowledgeRetention` indicates perfect performance in those areas, while other metrics like `METEOR` show a more moderate performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8c1ebf43d9ca67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:17:22.364933Z",
     "start_time": "2024-09-14T16:17:22.360554Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator_metrics_score = evaluator.metrics_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798029889742b9c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:17:22.956195Z",
     "start_time": "2024-09-14T16:17:22.946022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness': 0.93,\n",
       " 'AnswerRelevancy': 1.0,\n",
       " 'ContextualRelevancy': 1.0,\n",
       " 'GEval': 0.8,\n",
       " 'Hallucination': 0.0,\n",
       " 'KnowledgeRetention': 1.0,\n",
       " 'precision': 0.74,\n",
       " 'recall': 0.77,\n",
       " 'f1_score': 0.75,\n",
       " 'METEOR': 0.76,\n",
       " 'evaluation_score': 0.97}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_metrics_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd339ed8bc5be6",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Plotting Evaluation Metrics:** This line generates visual plots of the evaluation metrics using the `plot` method of the `evaluator` object. The plots provide a graphical representation of the model's performance across different metrics, making it easier to analyze and compare the results.\n",
    "\n",
    "- **Dash UI Interface:** When `mode=\"external\"` is used, this will open a Dash UI in a new browser window or tab to display the evaluation metrics plots interactively.\n",
    "\n",
    "- **Colab Users:** If you are using Google Colab, it is recommended to set `mode=\"inline\"` instead. This will render the plots directly within the notebook, making it more convenient for users working in an online environment like Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ca2f3ea94d1ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:17:47.756630Z",
     "start_time": "2024-09-14T16:17:44.306027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "evaluator.plot(mode=\"external\",interpreter=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45da948538931d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
