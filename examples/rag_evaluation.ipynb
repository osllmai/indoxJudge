{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG Evaluator",
   "id": "683cbd8af3980e07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDoxJudge/blob/main/examples/llm_evaluation.ipynb)",
   "id": "5413162f477c5781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install indoxJudge -U\n",
    "!pip install transformers\n",
    "!pip install torch"
   ],
   "id": "59547c8b03b3fd98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxJudge\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxJudge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxJudge\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxJudge/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ],
   "id": "c259ce9e5cffc8c0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-10T16:14:15.590744Z",
     "start_time": "2024-08-10T16:14:15.586224Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T16:14:31.970337Z",
     "start_time": "2024-08-10T16:14:31.959805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are the benefits of diet?\"\n",
    "retrieval_context = [\"The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.\",\"Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer's disease. The diet's high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.\",\"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition.\"]\n",
    "response = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer's disease, promoting longevity and overall well-being.\""
   ],
   "id": "c243c048e503e90f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Importing Required Modules\n",
    "imports the necessary classes from the indoxJudge library. The RAGevaluator class is used for evaluating retrieval-augmented generation models, assessing their performance based on various metrics. Additionally, OpenAi is the class used to interact with OpenAI models, such as GPT-3.5, providing the foundational language model capabilities for evaluation."
   ],
   "id": "65529ced4f6beb0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T16:13:24.282503Z",
     "start_time": "2024-08-10T16:13:20.700646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indoxJudge.piplines import RagEvaluator\n",
    "from indoxJudge.models import OpenAi"
   ],
   "id": "57511dda37de77a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing the OpenAI Model\n",
    "Here, the OpenAi class is instantiated to create a model object that interacts with OpenAI's gpt-3.5-turbo-0125 model. The api_key is passed to authenticate the API request. Replace OPENAI_API_KEY with your actual API key."
   ],
   "id": "14599bd20a3d8f5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T16:14:19.537912Z",
     "start_time": "2024-08-10T16:14:19.350130Z"
    }
   },
   "cell_type": "code",
   "source": "model = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-3.5-turbo-0125\")",
   "id": "b8f0ad7b798f17c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing OpenAi with model: gpt-3.5-turbo-0125\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The RAGevaluator class is instantiated to create an evaluator object. This object is configured to assess the modelâ€™s performance by using a specific response, retrieval context, and query. The llm_as_judge parameter is set to the model created in the previous step, while llm_response, retrieval_context, and query are additional components utilized during the evaluation process.",
   "id": "bf9b557f91707091"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T16:14:40.948169Z",
     "start_time": "2024-08-10T16:14:37.134335Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator = RagEvaluator(llm_as_judge=model,llm_response=response,retrieval_context=retrieval_context,query=query)",
   "id": "53d8bf90524259fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRagEvaluator initialized with model and metrics.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mModel set for all metrics.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Running the Evaluation:** This line calls the `judge` method on the `evaluator` object. The `judge` method runs through all the specified metrics (e.g., Faithfulness, Answer Relevancy, Hallucination, GEval, Knowledge Retention, BertScore, METEOR) to evaluate the language model's performance.\n",
    "  \n",
    "- **Logging the Process:** As the evaluation runs, the process logs the start and completion of each metric evaluation, providing feedback on the progress. Each log entry is tagged with an INFO level, indicating routine operational messages.\n",
    "  \n",
    "- **Handling Warnings:** You may notice a warning regarding model initialization and a future deprecation notice from the Hugging Face Transformers library. These warnings inform you about potential issues related to model compatibility and upcoming changes in the library.\n",
    "  \n",
    "- **Completion of Metrics:** After all metrics have been evaluated, the `judge` method completes, and the evaluation results are stored in the `eval_result` dictionary for further analysis or reporting.\n",
    "\n"
   ],
   "id": "b11a2ae0a78d3b70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T16:17:03.608053Z",
     "start_time": "2024-08-10T16:16:59.209377Z"
    }
   },
   "cell_type": "code",
   "source": "eval_result = evaluator.judge()",
   "id": "af324b3695562eb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Faithfulness\u001B[0m\n",
      "\u001B[32mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[31mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[32mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[31mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[32mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[31mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[32mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n",
      "\u001B[31mERROR\u001B[0m: \u001B[31m\u001B[1mError generating response: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m eval_result \u001B[38;5;241m=\u001B[39m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjudge\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Projects\\Nerd_studio\\indoxJudge\\indoxJudge\\piplines\\ragEvaluator\\rag_evaluator.py:83\u001B[0m, in \u001B[0;36mRagEvaluator.judge\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     81\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating metric: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(metric, Faithfulness):\n\u001B[1;32m---> 83\u001B[0m     claims \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_claims\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m     truths \u001B[38;5;241m=\u001B[39m metric\u001B[38;5;241m.\u001B[39mevaluate_truths()\n\u001B[0;32m     85\u001B[0m     verdicts \u001B[38;5;241m=\u001B[39m metric\u001B[38;5;241m.\u001B[39mevaluate_verdicts(claims\u001B[38;5;241m.\u001B[39mclaims)\n",
      "File \u001B[1;32mD:\\Projects\\Nerd_studio\\indoxJudge\\indoxJudge\\metrics\\faithfulness\\faithfulness.py:76\u001B[0m, in \u001B[0;36mFaithfulness.evaluate_claims\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03mEvaluates and extracts claims from the LLM response.\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \n\u001B[0;32m     73\u001B[0m \u001B[38;5;124;03m:return: A Claims object containing the list of claims.\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     75\u001B[0m prompt \u001B[38;5;241m=\u001B[39m FaithfulnessTemplate\u001B[38;5;241m.\u001B[39mgenerate_claims(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_response)\n\u001B[1;32m---> 76\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_language_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m claims \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response)\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclaims\u001B[39m\u001B[38;5;124m'\u001B[39m, [])\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Claims(claims\u001B[38;5;241m=\u001B[39mclaims)\n",
      "File \u001B[1;32mD:\\Projects\\Nerd_studio\\indoxJudge\\indoxJudge\\metrics\\faithfulness\\faithfulness.py:140\u001B[0m, in \u001B[0;36mFaithfulness._call_language_model\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_language_model\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    134\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m    Calls the language model with the given prompt and returns the response.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \n\u001B[0;32m    137\u001B[0m \u001B[38;5;124;03m    :param prompt: The prompt to provide to the language model.\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;124;03m    :return: The response from the language model.\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 140\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_evaluation_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mD:\\Projects\\Nerd_studio\\indoxJudge\\indoxJudge\\models\\openai.py:85\u001B[0m, in \u001B[0;36mOpenAi.generate_evaluation_response\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     81\u001B[0m     messages \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     82\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are an assistant for LLM evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m     83\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt},\n\u001B[0;32m     84\u001B[0m     ]\n\u001B[1;32m---> 85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m150\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     87\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError generating response to custom prompt: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\indoxjudge\\lib\\site-packages\\tenacity\\__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[1;34m(*args, **kw)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[1;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(f, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\indoxjudge\\lib\\site-packages\\tenacity\\__init__.py:389\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoSleep):\n\u001B[0;32m    388\u001B[0m     retry_state\u001B[38;5;241m.\u001B[39mprepare_for_next_attempt()\n\u001B[1;32m--> 389\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m do\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\indoxjudge\\lib\\site-packages\\tenacity\\nap.py:31\u001B[0m, in \u001B[0;36msleep\u001B[1;34m(seconds)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msleep\u001B[39m(seconds: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     26\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \n\u001B[0;32m     29\u001B[0m \u001B[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseconds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Retrieving Metric Scores:** The first line assigns the detailed metric scores from the evaluation to the variable `evaluator_metrics_score`. These scores reflect the performance of the language model across individual metrics such as Faithfulness, Answer Relevancy, Hallucination, GEval, Knowledge Retention, BertScore, METEOR, Recall, F1 Score.\n",
    "\n",
    "- **Retrieving Overall Evaluation Score:** The second line assigns the overall evaluation score to the variable `evaluator_evaluation_score`. This score is typically a cumulative measure that reflects the model's performance across all evaluated metrics.\n",
    "\n",
    "- **Example Output:** The dictionary shown represents a typical output of `evaluator_metrics_score`. Each key corresponds to a specific metric, and the associated value indicates the model's score for that metric. For instance, a `1.0` score in `AnswerRelevancy` and `KnowledgeRetention` indicates perfect performance in those areas, while other metrics like `METEOR` show a more moderate performance.\n"
   ],
   "id": "225bcfef25a2bd5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:18:44.836394Z",
     "start_time": "2024-08-10T15:18:44.833366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator_metrics_score = evaluator.metrics_score\n",
    "evaluator_evaluation_score = evaluator.evaluation_score"
   ],
   "id": "7d8c1ebf43d9ca67",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:18:44.843036Z",
     "start_time": "2024-08-10T15:18:44.837400Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator_metrics_score",
   "id": "798029889742b9c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness': 1.0,\n",
       " 'AnswerRelevancy': 1.0,\n",
       " 'Bias': 0.0,\n",
       " 'Hallucination': 0.0,\n",
       " 'KnowledgeRetention': 1.0,\n",
       " 'Toxicity': 0.0,\n",
       " 'precision': 0.74,\n",
       " 'recall': 0.77,\n",
       " 'f1_score': 0.75,\n",
       " 'BLEU': 0.28,\n",
       " 'gruen': 0.72}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:18:44.847637Z",
     "start_time": "2024-08-10T15:18:44.844042Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator_evaluation_score",
   "id": "67cd9c6ee59b0e01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Plotting Evaluation Metrics:** This line generates visual plots of the evaluation metrics using the `plot` method of the `evaluator` object. The plots provide a graphical representation of the model's performance across different metrics, making it easier to analyze and compare the results.\n",
    "\n",
    "- **Dash UI Interface:** When `mode=\"external\"` is used, this will open a Dash UI in a new browser window or tab to display the evaluation metrics plots interactively.\n",
    "\n",
    "- **Colab Users:** If you are using Google Colab, it is recommended to set `mode=\"inline\"` instead. This will render the plots directly within the notebook, making it more convenient for users working in an online environment like Colab.\n"
   ],
   "id": "5bdd339ed8bc5be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:19:18.168424Z",
     "start_time": "2024-08-10T15:19:13.146998Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator.plot(mode=\"external\")",
   "id": "f1ca2f3ea94d1ee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d45da948538931d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
