{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683cbd8af3980e07",
   "metadata": {},
   "source": [
    "# LLM Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413162f477c5781",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDoxJudge/blob/main/examples/llm_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59547c8b03b3fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indoxJudge -U\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259ce9e5cffc8c0",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxJudge`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxJudge\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxJudge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxJudge\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxJudge/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:37:55.829754Z",
     "start_time": "2024-08-15T15:37:55.818434Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c243c048e503e90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:37:56.058713Z",
     "start_time": "2024-08-15T15:37:56.055613Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How does photosynthesis work in plants?\"\n",
    "\n",
    "retrieval_context = [\n",
    "    \"Photosynthesis is a biochemical process by which green plants, algae, and some bacteria convert light energy into chemical energy. \"\n",
    "    \"It primarily occurs in the chloroplasts of plant cells, which contain the green pigment chlorophyll that captures sunlight.\",\n",
    "    \n",
    "    \"The process begins with light-dependent reactions in the thylakoid membranes of the chloroplasts. These reactions use solar energy to split water molecules \"\n",
    "    \"(photolysis), producing oxygen gas (O2) as a byproduct and generating energy-rich molecules, ATP and NADPH.\",\n",
    "    \n",
    "    \"The second stage of photosynthesis, known as the Calvin cycle or light-independent reactions, takes place in the stroma of the chloroplast. \"\n",
    "    \"During this stage, the plant uses the ATP and NADPH from the light-dependent reactions to fix carbon dioxide (CO2) into glucose molecules through a series of enzymatic steps.\",\n",
    "    \n",
    "    \"Photosynthesis is influenced by several factors, including light intensity, carbon dioxide concentration, temperature, and availability of water. \"\n",
    "    \"Plants optimize these factors through adaptations such as leaf structure, stomatal control, and pigment types for efficient energy capture.\",\n",
    "    \n",
    "    \"This process is crucial for life on Earth because it not only provides the primary energy source for most ecosystems but also contributes to the oxygenation of the atmosphere, \"\n",
    "    \"maintaining a balance in the global carbon cycle and supporting the existence of aerobic life forms.\"\n",
    "]\n",
    "\n",
    "response = (\n",
    "    \"Photosynthesis is a fundamental process that allows plants to convert light energy into chemical energy, which is stored as glucose. \"\n",
    "    \"The process occurs in two stages: light-dependent reactions in the thylakoid membranes and the Calvin cycle in the stroma of the chloroplasts. \"\n",
    "    \"In the light-dependent reactions, sunlight splits water molecules to release oxygen and generate energy carriers like ATP and NADPH. \"\n",
    "    \"The Calvin cycle then uses these energy carriers to fix carbon dioxide into glucose. This process is critical for sustaining life on Earth by providing food and oxygen.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65529ced4f6beb0b",
   "metadata": {},
   "source": [
    "## Importing Required Modules\n",
    "imports the necessary classes from the indoxJudge library. LLMEvaluator is the class used for evaluating language models based on various metrics, and OpenAi is the class used to interact with the OpenAI models, such as GPT-3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57511dda37de77a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:37:58.598688Z",
     "start_time": "2024-08-15T15:37:56.513855Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxJudge.pipelines import LLMEvaluator\n",
    "from indoxJudge.models import OpenAi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14599bd20a3d8f5d",
   "metadata": {},
   "source": [
    "## Initializing the OpenAI Model\n",
    "Here, the OpenAi class is instantiated to create a model object that interacts with OpenAI's gpt-3.5-turbo-0125 model. The api_key is passed to authenticate the API request. Replace OPENAI_API_KEY with your actual API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f0ad7b798f17c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:37:59.153648Z",
     "start_time": "2024-08-15T15:37:58.599698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-4o-mini and max_tokens: 1024\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the language model\n",
    "# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o. or other models\n",
    "\n",
    "model = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o-mini\",max_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b557f91707091",
   "metadata": {},
   "source": [
    "the LLMEvaluator class is instantiated to create an evaluator object. This object is configured to evaluate the model using a specific response, retrieval context, and query. The llm_as_judge parameter is set to the model created in the previous cell, while llm_response, retrieval_context, and query are other components used during the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d8bf90524259fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:38:05.531002Z",
     "start_time": "2024-08-15T15:37:59.154155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluator initialized with model and metrics.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mModel set for all metrics.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluator = LLMEvaluator(llm_as_judge=model,llm_response=response,retrieval_context=retrieval_context,query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2ae0a78d3b70",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Running the Evaluation:** This line calls the `judge` method on the `evaluator` object. The `judge` method runs through all the specified metrics (e.g., Faithfulness, Answer Relevancy, Bias, Hallucination, Knowledge Retention, Toxicity, BertScore, BLEU, Gruen) to evaluate the language model's performance.\n",
    "  \n",
    "- **Logging the Process:** As the evaluation runs, the process logs the start and completion of each metric evaluation, providing feedback on the progress. Each log entry is tagged with an INFO level, indicating routine operational messages.\n",
    "  \n",
    "- **Handling Warnings:** You may notice a warning regarding model initialization and a future deprecation notice from the Hugging Face Transformers library. These warnings inform you about potential issues related to model compatibility and upcoming changes in the library.\n",
    "  \n",
    "- **Completion of Metrics:** After all metrics have been evaluated, the `judge` method completes, and the evaluation results are stored in the `eval_result` dictionary for further analysis or reporting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af324b3695562eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:38:52.426901Z",
     "start_time": "2024-08-15T15:38:05.532008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Faithfulness\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 292 | Output: 153 | Total: 445\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 257 | Output: 155 | Total: 412\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 975 | Output: 120 | Total: 1095\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 489 | Output: 33 | Total: 522\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 292 | Output: 153 | Total: 445\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 975 | Output: 120 | Total: 1095\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Faithfulness, score: 1.0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: AnswerRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 233 | Output: 155 | Total: 388\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 529 | Output: 155 | Total: 684\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 197 | Output: 35 | Total: 232\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: AnswerRelevancy, score: 1.0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Bias\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 285 | Output: 8 | Total: 293\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 214 | Output: 34 | Total: 248\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Bias, score: 0.0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Hallucination\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 759 | Output: 248 | Total: 1007\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 297 | Output: 37 | Total: 334\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Hallucination, score: 0.0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: KnowledgeRetention\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 311 | Output: 16 | Total: 327\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 597 | Output: 10 | Total: 607\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 144 | Output: 33 | Total: 177\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: KnowledgeRetention, score: 1.0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Toxicity\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 747 | Output: 84 | Total: 831\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 747 | Output: 84 | Total: 831\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Toxicity, score: 0.0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BertScore\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m\n",
      "Completed evaluation for metric: BertScore, scores: \n",
      "precision: 0.67,\n",
      "recall: 0.74,\n",
      "f1_score: 0.7,\n",
      "                        \u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BLEU\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: BLEU, score: 0.17\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Gruen\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Gruen, score: 0.73\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.judge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225bcfef25a2bd5c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Retrieving Metric Scores:** The first line assigns the detailed metric scores from the evaluation to the variable `evaluator_metrics_score`. These scores reflect the performance of the language model across individual metrics such as Answer Relevancy, Bias, Knowledge Retention, Toxicity, Precision, Recall, F1 Score, BLEU, and Gruen.\n",
    "\n",
    "- **Retrieving Overall Evaluation Score:** The second line assigns the overall evaluation score to the variable `evaluator_evaluation_score`. This score is typically a cumulative measure that reflects the model's performance across all evaluated metrics.\n",
    "\n",
    "- **Example Output:** The dictionary shown represents a typical output of `evaluator_metrics_score`. Each key corresponds to a specific metric, and the associated value indicates the model's score for that metric. For instance, a `1.0` score in `AnswerRelevancy` and `KnowledgeRetention` indicates perfect performance in those areas, while other metrics like `BLEU` and `gruen` show a more moderate performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8c1ebf43d9ca67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:51:16.556364Z",
     "start_time": "2024-08-13T15:51:16.550458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness': 0.93,\n",
       " 'AnswerRelevancy': 1.0,\n",
       " 'Bias': 0.0,\n",
       " 'Hallucination': 0.0,\n",
       " 'KnowledgeRetention': 1.0,\n",
       " 'Toxicity': 0.0,\n",
       " 'precision': 0.74,\n",
       " 'recall': 0.77,\n",
       " 'f1_score': 0.75,\n",
       " 'BLEU': 0.3,\n",
       " 'Gruen': 0.72,\n",
       " 'evaluation_score': 0.92}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_metrics_score = evaluator.metrics_score\n",
    "evaluator_metrics_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd339ed8bc5be6",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Plotting Evaluation Metrics:** This line generates visual plots of the evaluation metrics using the `plot` method of the `evaluator` object. The plots provide a graphical representation of the model's performance across different metrics, making it easier to analyze and compare the results.\n",
    "\n",
    "- **Dash UI Interface:** When `mode=\"external\"` is used, this will open a Dash UI in a new browser window or tab to display the evaluation metrics plots interactively.\n",
    "\n",
    "- **Colab Users:** If you are using Google Colab, it is recommended to set `mode=\"inline\"` instead. This will render the plots directly within the notebook, making it more convenient for users working in an online environment like Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ca2f3ea94d1ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:51:35.089019Z",
     "start_time": "2024-08-13T15:51:32.994589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "evaluator.plot(mode=\"external\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45da948538931d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
