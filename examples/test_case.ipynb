{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:53.132289Z",
     "start_time": "2024-07-24T08:49:53.119738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "INDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")"
   ],
   "id": "ed950b5f2833500a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:53.620255Z",
     "start_time": "2024-07-24T08:49:53.133121Z"
    }
   },
   "source": [
    "from indox.IndoxEval.llms import IndoxApi\n",
    "llm = IndoxApi(api_key=INDOX_API_KEY)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:21:13.247996Z",
     "start_time": "2024-07-25T08:21:13.242374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "now i want to create a markdown documentation for each one of them.\n",
    "the structure is like this :\n",
    "first introduce the class of function with its hyperparameters,\n",
    "for example :\n",
    "    class Faithfulness:\n",
    "    \"\"\"\n",
    "    Class for evaluating the faithfulness of language model outputs by analyzing\n",
    "    claims, truths, verdicts, and reasons using a specified language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_response, retrieval_context):\n",
    "        \"\"\"\n",
    "        Initializes the Faithfulness class with the LLM response and retrieval context.\n",
    "\n",
    "        :param llm_response: The response generated by the language model.\n",
    "        :param retrieval_context: The context used for retrieval during evaluation.\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.llm_response = llm_response\n",
    "        self.retrieval_context = retrieval_context\n",
    "then explain the function of each hyperparameter and what it does.\n",
    "after that the usage and  example\n",
    "\n",
    "\n",
    "for example :\n",
    "    import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "from indox.IndoxEval.llms import OpenAi\n",
    "llm = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-3.5-turbo\")\n",
    "    from indox.IndoxEval import AnswerRelevancy\n",
    "answer_relevancy_metric = AnswerRelevancy(query=query,llm_response=llm_response)\n",
    "\n",
    "from indox.IndoxEval import Evaluator\n",
    "evaluator = Evaluator(model=llm,metrics=[answer_relevancy_metric])\n",
    "and explain steps of usage and example\n",
    "\n",
    "now i want to send you each one and you give me markdown format of document\n"
   ],
   "id": "36e451b331202b53",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2209518842.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    now i want to create a markdown documentation for each one of them.\u001B[0m\n\u001B[1;37m        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.096687Z",
     "start_time": "2024-07-24T08:49:54.093084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "test_case = {\n",
    "                    \"messages\": [\n",
    "                        {\"query\": \"What are the best books for learning Python?\",\n",
    "                         \"llm_response\": \"Automate the Boring Stuff with Python, Python Crash Course, etc.\"},\n",
    "                        {\"query\": \"What is the capital of France?\", \"llm_response\": \"The capital of France is Paris.\"}\n",
    "                    ]\n",
    "                }"
   ],
   "id": "5e799a33363755e9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.427198Z",
     "start_time": "2024-07-24T08:49:54.423147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"llm_response\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can you tell me about the Eiffel Tower?\",\n",
    "        \"llm_response\": \"The Eiffel Tower, located in Paris, is one of the most iconic landmarks in the world.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are some famous foods in France?\",\n",
    "        \"llm_response\": \"France is known for its cuisine, including dishes like croissants, baguettes, and cheese.\"\n",
    "    }\n",
    "]"
   ],
   "id": "1cb70816e2109925",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.838628Z",
     "start_time": "2024-07-24T08:49:54.835267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What is the capital of Japan?\"\n",
    "llm_response = \"The capital of Japan is Tokyo.\"\n",
    "retrieval_context = [\"Tokyo is the most populous city in Japan and serves as the country's political and economic center.\",\n",
    "                     \"Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.\",\n",
    "                     \"The city is known for its mix of modern architecture and traditional temples, as well as its bustling districts like Shibuya and Shinjuku.\"]"
   ],
   "id": "983cd49670fe3394",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.227679Z",
     "start_time": "2024-07-24T08:49:55.223760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Faithfulness\n",
    "faithfulness_evaluator = Faithfulness(llm_response=llm_response,retrieval_context=retrieval_context)"
   ],
   "id": "741f1c522cf4a890",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.527497Z",
     "start_time": "2024-07-24T08:49:55.523784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import AnswerRelevancy\n",
    "answer_relevancy_metric = AnswerRelevancy(query=query,llm_response=llm_response)"
   ],
   "id": "383c8eb91a2c2532",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.827986Z",
     "start_time": "2024-07-24T08:49:55.824382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Bias\n",
    "bias_metric = Bias(llm_response=llm_response)"
   ],
   "id": "ce1de7123674cce6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.104118Z",
     "start_time": "2024-07-24T08:49:56.101015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import ContextualRelevancy\n",
    "contextual = ContextualRelevancy(query=query,retrieval_context=retrieval_context)"
   ],
   "id": "d3e25716ada96525",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.408440Z",
     "start_time": "2024-07-24T08:49:56.404515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import GEval\n",
    "geval = GEval(parameters=\"Rag application\",query=query,llm_response=llm_response,ground_truth=\"Tokyo\",context=\"geographic knowledge\",\n",
    "              retrieval_context=retrieval_context)"
   ],
   "id": "16c2d17a25f3a714",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.654507Z",
     "start_time": "2024-07-24T08:49:56.651101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import KnowledgeRetention\n",
    "knowledge_retention = KnowledgeRetention(messages=test_messages)"
   ],
   "id": "73cd7a7ecae55215",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.928481Z",
     "start_time": "2024-07-24T08:49:56.925478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Hallucination\n",
    "hallucination = Hallucination(llm_response=llm_response,retrieval_context=retrieval_context)"
   ],
   "id": "843c43180fbc887c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:57.229825Z",
     "start_time": "2024-07-24T08:49:57.226918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Toxicity\n",
    "toxicity = Toxicity(messages=test_messages)"
   ],
   "id": "9f34c622dbf50063",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:57.591474Z",
     "start_time": "2024-07-24T08:49:57.587061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Evaluator\n",
    "evaluator = Evaluator(model=llm,metrics=[contextual,faithfulness_evaluator,answer_relevancy_metric,\n",
    "                                         hallucination,toxicity,geval,knowledge_retention,bias_metric])"
   ],
   "id": "ddeacee6e56a6cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluator initialized with model and metrics.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mModel set for all metrics.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:50:51.680754Z",
     "start_time": "2024-07-24T08:49:57.959790Z"
    }
   },
   "cell_type": "code",
   "source": "result = evaluator.evaluate()",
   "id": "4ff8285425030450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: ContextualRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: ContextualRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Faithfulness\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Faithfulness\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: AnswerRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: AnswerRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Hallucination\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Hallucination\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Toxicity\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Toxicity\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: GEval\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: GEval\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: KnowledgeRetention\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: KnowledgeRetention\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Bias\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Bias\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:50:51.688588Z",
     "start_time": "2024-07-24T08:50:51.681760Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "c1f3abd628e26a79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contextual_relevancy': {'verdicts': [{'verdict': 'yes',\n",
       "    'reason': 'No reason provided'},\n",
       "   {'verdict': 'yes', 'reason': 'No reason provided'},\n",
       "   {'verdict': 'no',\n",
       "    'reason': 'The context provided does not mention anything about the capital of Japan or provide any relevant information related to the question.'}],\n",
       "  'reason': {'reason': 'The score is 0 because the context provided does not contain any information about the capital of Japan, which is Tokyo.'}},\n",
       " 'faithfulness': {'claims': ['The capital of Japan is Tokyo.'],\n",
       "  'truths': ['The capital of Japan is Tokyo.'],\n",
       "  'verdicts': [{'verdict': 'yes', 'reason': None}],\n",
       "  'reason': 'The score is 1 because the actual output perfectly aligns with the retrieval context, stating that the capital of Japan is Tokyo.'},\n",
       " 'answer_relevancy': {'score': 1,\n",
       "  'reason': 'The score is 1.00 because the answer is perfectly relevant to the input provided. Great job!',\n",
       "  'statements': [],\n",
       "  'verdicts': []},\n",
       " 'hallucination': {'score': 0.6666666666666666,\n",
       "  'reason': 'The score is 0.67 because there are contradictions in the actual output compared to the provided context, indicating some level of hallucination in the generated content.',\n",
       "  'verdicts': [{'verdict': 'yes',\n",
       "    'reason': \"The actual output agrees with the provided context which states that Tokyo is the most populous city in Japan and serves as the country's political and economic center.\"},\n",
       "   {'verdict': 'no',\n",
       "    'reason': 'The actual output contradicts the provided context which states that Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.'},\n",
       "   {'verdict': 'no',\n",
       "    'reason': \"The actual output lacks detail and does not contradict the provided context which describes Tokyo's architecture and districts.\"}]},\n",
       " 'toxicity': {'score': 0,\n",
       "  'reason': 'The score is 0.00 because there are no reasons provided for toxicity, indicating a non-toxic output.',\n",
       "  'opinions': [],\n",
       "  'verdicts': []},\n",
       " 'geval': '{\\n    \"score\": 8,\\n    \"reason\": \"The response is relevant and accurate, but lacks comprehensiveness in covering all key points and contextuality from the Rag application.\"\\n}',\n",
       " 'knowledge_retention': {'score': 1.0,\n",
       "  'reason': 'The score is 1.00 because there are no attritions indicating forgetfulness, ensuring strong knowledge retention.',\n",
       "  'verdicts': [{'index': 0, 'verdict': 'no', 'reason': None},\n",
       "   {'index': 1, 'verdict': 'no', 'reason': None},\n",
       "   {'index': 2, 'verdict': 'no', 'reason': None}],\n",
       "  'knowledges': [{'Capital of France': 'Paris'},\n",
       "   {'Capital of France': 'Paris', 'Landmark': 'Eiffel Tower'},\n",
       "   {'Capital of France': 'Paris',\n",
       "    'Landmark': 'Eiffel Tower',\n",
       "    'Famous foods in France': []}]},\n",
       " 'bias': {'score': 0,\n",
       "  'reason': 'The score is 0.00 because there are no identified reasons for bias in the actual output.',\n",
       "  'opinions': [],\n",
       "  'verdicts': []}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{'contextual_relevancy': {'verdicts': [{'verdict': 'yes',\n",
    "    'reason': 'No reason provided'},\n",
    "   {'verdict': 'yes', 'reason': 'No reason provided'},\n",
    "   {'verdict': 'no',\n",
    "    'reason': 'The context provided does not mention anything about the capital of Japan or provide any relevant information related to the question.'}],\n",
    "  'reason': {'reason': 'The score is 0 because the context provided does not contain any information about the capital of Japan, which is Tokyo.'}},\n",
    " 'faithfulness': {'claims': ['The capital of Japan is Tokyo.'],\n",
    "  'truths': ['The capital of Japan is Tokyo.'],\n",
    "  'verdicts': [{'verdict': 'yes', 'reason': None}],\n",
    "  'reason': 'The score is 1 because the actual output perfectly aligns with the retrieval context, stating that the capital of Japan is Tokyo.'},\n",
    " 'answer_relevancy': {'score': 1,\n",
    "  'reason': 'The score is 1.00 because the answer is perfectly relevant to the input provided. Great job!',\n",
    "  'statements': [],\n",
    "  'verdicts': []},\n",
    " 'hallucination': {'score': 0.6666666666666666,\n",
    "  'reason': 'The score is 0.67 because there are contradictions in the actual output compared to the provided context, indicating some level of hallucination in the generated content.',\n",
    "  'verdicts': [{'verdict': 'yes',\n",
    "    'reason': \"The actual output agrees with the provided context which states that Tokyo is the most populous city in Japan and serves as the country's political and economic center.\"},\n",
    "   {'verdict': 'no',\n",
    "    'reason': 'The actual output contradicts the provided context which states that Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.'},\n",
    "   {'verdict': 'no',\n",
    "    'reason': \"The actual output lacks detail and does not contradict the provided context which describes Tokyo's architecture and districts.\"}]},\n",
    " 'toxicity': {'score': 0,\n",
    "  'reason': 'The score is 0.00 because there are no reasons provided for toxicity, indicating a non-toxic output.',\n",
    "  'opinions': [],\n",
    "  'verdicts': []},\n",
    " 'geval': '{\\n    \"score\": 8,\\n    \"reason\": \"The response is relevant and accurate, but lacks comprehensiveness in covering all key points and contextuality from the Rag application.\"\\n}',\n",
    " 'knowledge_retention': {'score': 1.0,\n",
    "  'reason': 'The score is 1.00 because there are no attritions indicating forgetfulness, ensuring strong knowledge retention.',\n",
    "  'verdicts': [{'index': 0, 'verdict': 'no', 'reason': None},\n",
    "   {'index': 1, 'verdict': 'no', 'reason': None},\n",
    "   {'index': 2, 'verdict': 'no', 'reason': None}],\n",
    "  'knowledges': [{'Capital of France': 'Paris'},\n",
    "   {'Capital of France': 'Paris', 'Landmark': 'Eiffel Tower'},\n",
    "   {'Capital of France': 'Paris',\n",
    "    'Landmark': 'Eiffel Tower',\n",
    "    'Famous foods in France': []}]},\n",
    " 'bias': {'score': 0,\n",
    "  'reason': 'The score is 0.00 because there are no identified reasons for bias in the actual output.',\n",
    "  'opinions': [],\n",
    "  'verdicts': []}}"
   ],
   "id": "6a10933510c81655"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
