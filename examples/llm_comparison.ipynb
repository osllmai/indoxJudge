{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLMComparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDoxJudge/blob/main/examples/llm_comparison.ipynb)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Defining Model Data:** The `models` list contains dictionaries representing three different language models, each with a name, overall score, and detailed evaluation metrics. These metrics include Faithfulness, Answer Relevancy, Bias, Hallucination, Knowledge Retention, Toxicity, Precision, Recall, F1 Score, and BLEU. The `score` key represents the overall evaluation score for each model.\n",
    "\n",
    "- **Importing LLMComparison:** The `LLMComparison` class from the `indoxJudge.piplines` module is imported. This class is used to compare the performance of the different language models based on their metrics.\n",
    "\n",
    "- **Initializing the Comparison:** An instance of `LLMComparison` is created by passing the `models` list to it. This instance, `llm_comparison`, will handle the comparison of the models.\n",
    "\n",
    "- **Plotting the Comparison:** The `plot` method is called with `mode=\"inline\"` to generate and display a comparative visualization of the models' performance within the notebook. This is especially useful for users working in environments like Google Colab, where inline plotting is preferred for ease of use.\n",
    "\n",
    "This cell is designed to compare multiple language models visually, allowing for a detailed analysis of their respective strengths and weaknesses across various metrics.\n"
   ],
   "id": "f78d284f7aaa4b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install indoxJudge -U",
   "id": "ba1566148beae1d9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-10T15:34:33.972500Z",
     "start_time": "2024-08-10T15:34:33.967625Z"
    }
   },
   "source": [
    "models = [{'name': 'Model_1',\n",
    "  'score': 0.50,\n",
    "  'metrics': {'Faithfulness': 0.55,\n",
    "   'AnswerRelevancy': 1.0,\n",
    "   'Bias': 0.45,\n",
    "   'Hallucination': 0.8,\n",
    "   'KnowledgeRetention': 0.0,\n",
    "   'Toxicity': 0.0,\n",
    "   'precision': 0.64,\n",
    "   'recall': 0.77,\n",
    "   'f1_score': 0.70,\n",
    "   'BLEU': 0.11}},\n",
    " {'name': 'Model_2',\n",
    "  'score': 0.61,\n",
    "  'metrics': {'Faithfulness': 1.0,\n",
    "   'AnswerRelevancy': 1.0,\n",
    "   'Bias': 0.0,\n",
    "   'Hallucination': 0.8,\n",
    "   'KnowledgeRetention': 1.0,\n",
    "   'Toxicity': 0.0,\n",
    "   'precision': 0.667,\n",
    "   'recall': 0.77,\n",
    "   'f1_score': 0.71,\n",
    "   'BLEU': 0.14}},\n",
    " {'name': 'Model_3',\n",
    "  'score': 0.050,\n",
    "  'metrics': {'Faithfulness': 1.0,\n",
    "   'AnswerRelevancy': 1.0,\n",
    "   'Bias': 0.0,\n",
    "   'Hallucination': 0.83,\n",
    "   'KnowledgeRetention': 0.0,\n",
    "   'Toxicity': 0.0,\n",
    "   'precision': 0.64,\n",
    "   'recall': 0.76,\n",
    "   'f1_score': 0.70,\n",
    "   'BLEU': 0.10}},\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from indoxJudge.piplines import LLMComparison\n",
    "llm_comparison = LLMComparison(models=models)\n",
    "llm_comparison.plot(mode=\"inline\")"
   ],
   "id": "d05a8007498795"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
