# METEOR

Class for evaluating the similarity between a generated response and one or more reference contexts using the METEOR metric, which considers precision, recall, and fragmentation.

## Initialization

The `METEOR` class is initialized with the following parameters:

- **llm_response**: The response generated by a language model.
- **retrieval_context**: The reference context(s) to compare against the actual response.

```python
class METEOR:
    def __init__(self, llm_response: str, retrieval_context: Union[str, List[str]]):
        """
        Initialize the METEOR evaluator.

        Parameters:
        llm_response (str): The response generated by the Language Model.
        retrieval_context (Union[str, List[str]]): The reference context(s) against which the response is evaluated.
        """
        self.llm_response = llm_response
        self.retrieval_context = retrieval_context
```
## Parameters Explanation

- **llm_response**: The actual response generated by the language model that needs to be evaluated.

- **retrieval_context**: The reference contexts for comparison. Can be a single string or a list of strings.

## Usage Example

Here is an example of how to use the `METEOR` class:

```python
from indoxJudge.metrics import METEOR
from indoxJudge.pipelines import Evaluator

# Define a sample response and context
llm_response = "The quick brown fox jumps over the lazy dog."
retrieval_context = [
    "The fast brown fox leaps over the lazy dog.",
    "A speedy brown fox jumps over a sleepy dog."
]

# Initialize the METEOR object
meteor = METEOR(
    llm_response=llm_response,
    retrieval_context=retrieval_context
)

# Measure the METEOR score
evaluator = Evaluator(model=None, metrics[meteor])
result = evaluator.judge()
```