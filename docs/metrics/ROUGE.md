# Rouge

Class for evaluating the similarity between a generated response and one or more expected responses using the ROUGE metric, which considers n-gram overlaps for recall and precision.

## Initialization

The `Rouge` class is initialized with the following parameters:

- **llm_response**: The response generated by a language model.
- **retrieval_context**: The expected response(s) to compare against the actual response.

```python
class Rouge:
    def __init__(
        self, llm_response: str, retrieval_context: Union[str, List[str]], n: int = 1
    ):
        """
        Initialize the Rouge evaluator with the desired n-gram size.

        Parameters:
        llm_response (str): The response generated by a language model.
        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.
        n (int): The size of the n-grams to use for evaluation (e.g., 1 for unigrams, 2 for bigrams, etc.).
        """
```
## Parameters Explanation

- **llm_response**: The actual response generated by the language model that needs to be evaluated.

- **retrieval_context**: The expected responses for comparison. Can be a single string or a list of strings.


## Usage Example

Here is an example of how to use the `Rouge` class:

```python
from indoxJudge.metrics import Rouge

# Define a sample response and context"
llm_response = "The quick brown fox jumps over the lazy dog."

retrieval_context = [
    "The fast brown fox leaps over the lazy dog.",
    "A speedy brown fox jumps over a sleepy dog."
]

# Initialize the Rouge object
rouge = Rouge(
    llm_response=llm_response,
    retrieval_context=retrieval_context,
)

# Measure the ROUGE score
result = rouge.measure()
```