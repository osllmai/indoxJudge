# Rouge

Class for evaluating the similarity between a generated response and one or more expected responses using the ROUGE metric, which considers n-gram overlaps for recall and precision.

## Initialization

The `Rouge` class is initialized with the following parameters:

- **llm_response**: The response generated by a language model.
- **retrieval_context**: The expected response(s) to compare against the actual response.
- **n**: The size of the n-grams to use for evaluation (e.g., 1 for unigrams, 2 for bigrams, etc.).

```python
from collections import Counter
from typing import Union, List, Tuple, Dict
import numpy as np
from indox.IndoxEval.utils import TextPreprocessor

class Rouge:
    def __init__(
        self, llm_response: str, retrieval_context: Union[str, List[str]], n: int = 1
    ):
        """
        Initialize the Rouge evaluator with the desired n-gram size.

        Parameters:
        llm_response (str): The response generated by a language model.
        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.
        n (int): The size of the n-grams to use for evaluation (e.g., 1 for unigrams, 2 for bigrams, etc.).
        """
```
## Parameters Explanation

- **llm_response**: The actual response generated by the language model that needs to be evaluated.

- **retrieval_context**: The expected responses for comparison. Can be a single string or a list of strings.

- **n**: The size of the n-grams to use for evaluation. Default is `1`.

## Usage Example

Here is an example of how to use the `Rouge` class:

```python
from indox.IndoxEval import Rouge, Evaluator

# Define a sample response and context"
retrieval_context = [
    "The fast brown fox leaps over the lazy dog.",
    "A speedy brown fox jumps over a sleepy dog."
]

# Initialize the Rouge object
rouge = Rouge(
    llm_response=llm_response,
    retrieval_context=retrieval_context,
    n=2
)

# Measure the ROUGE score
evaluator = Evaluator(model=None, metrics[rouge])
result = evaluator.evaluate()
```