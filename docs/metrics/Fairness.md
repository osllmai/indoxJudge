
# Fairness

Class for evaluating the fairness of language model outputs by analyzing the fairness score, reasons, and verdicts using a specified language model.

## Initialization

The `Fairness` class is initialized with the following parameters:

- **input_sentence**: The response generated by the language model.

```python
class Fairness:
    """
    Class for evaluating the fairness of language model outputs by analyzing
    the fairness score, reasons, and verdicts using a specified language model.
    """
    def __init__(self, input_sentence: str):
        """
        Initializes the Fairness class with the input sentence to be evaluated.

        :param input_sentence: The sentence to be evaluated for fairness.
        """
```

# Hyperparameters Explanation

- **input_sentence**: The response from the language model that needs to be evaluated for fairness.

# Usage Example

Here is an example of how to use the `Fairness` class:

```python
from indoxJudge.pipelines import SafetyEvaluator

# Define the input sentence to be evaluated
input_sentence = "Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently."

# Initialize the safety model with the provided input sentence
safety_model = SafetyEvaluator(model=model, input=input_sentence)

# Judge the metrics for fairness
metrics_score, metrics_reasons = safety_model.judge()

# Transform and print the metrics
transformed_metrics = safety_model.transform_metrics()

print("Metrics Scores:")
print(json.dumps(metrics_score, indent=4))

print("\nMetrics Reasons:")
print(json.dumps(metrics_reasons, indent=4))

print("Transformed Metrics:", transformed_metrics)
```
